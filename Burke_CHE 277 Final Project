#Import All Libraries
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.multioutput import MultiOutputRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score,confusion_matrix, ConfusionMatrixDisplay, classification_report
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
import seaborn as sns
import torch
import torch.nn as nn
import torch.optim as optim

#==============DATA IMPORTING AND PROCESSING ===============
#IMPORT DATA FROM FILES
BA_Data = pd.read_csv ("/file path/ba_raw_data.csv" , index_col =0,header =0)
MMD_Data=pd.read_csv ("/file path/mmd_raw_data.csv" , index_col =0,header =0)
MN_Data=pd.read_csv ("/file path/mn-mw_raw_data.csv" , index_col =0,header =0)
RT_Data=pd.read_csv ("/file path/rt_raw_data.csv" , index_col =0,header =0)
VA_Data=pd.read_csv ("/file path/Full_VA_Data.csv" , index_col =0,header =0)

#___________________________________
#ORGANIZE DATA INTO USEFUL VARIABLES

features = RT_Data.drop(columns=['reaction time'])  # Butyl Acrylate Features; Monomer, Initiator, Temperature

#Consolidates all BA data into one dataframe; shape 432x37
RT_y = RT_Data[['reaction time']]  # BA reaction time
MW_y = MN_Data.drop(columns=['AIBN-0', 'Temperature', 'BA-0'])  # BA molecular weight by both Number average and Weighted average Molecular weight
BA_y = BA_Data.drop(columns=['AIBN-0', 'Temperature', 'BA-0']) #BA concentrations
Reaction_data=MW_y
Reaction_data.insert(0,'Reaction Time',RT_y)
Reaction_data=pd.concat([Reaction_data,BA_y],axis=1) 

#MMD (Molecular Mass Distribution) ANALYSIS
# Adjust Data for BA
BA_Features=MMD_Data[['BA-0','AIBN-0', 'Temparature','time']]# Butyl Acrylate MMD  FEATURES SPECIFICALLY
BA_Features['Label'] = 'BA' #Add label to all Butyl Acrylate Data
BA_Features.columns = [ 'Monomer', 'Initiator','Temperature','Time','Label'] # Column titles for features
BA_MMD_Data=MMD_Data.drop(columns=['BA-0','AIBN-0', 'Temparature','time']) # we are left with the raw MMD data for 
BA_MMD_Data.columns= ['0', '0.08', '0.16', '0.24', '0.32', '0.4', '0.48', '0.56', '0.64', '0.72', '0.8','0.88', '0.96', '1.04', '1.12', '1.2', '1.28', 
    '1.36', '1.44', '1.52', '1.6','1.68', '1.76', '1.84', '1.92', '2.0', '2.08', '2.16', '2.24', '2.32', '2.4','2.48', '2.56', '2.64',
    '2.72', '2.80', '2.88', '2.96', '3.04', '3.12', '3.2','3.28', '3.36', '3.44', '3.52', '3.6', '3.68', '3.76', '3.84', '3.92', '4.0',
    '4.08', '4.16', '4.24', '4.32', '4.4', '4.48', '4.56', '4.64', '4.72', '4.8','4.88', '4.96', '5.04', '5.12', '5.2', '5.28', '5.36', 
    '5.44', '5.52', '5.6', '5.68', '5.76', '5.84', '5.92', '6.0', '6.08', '6.16', '6.24', '6.32', '6.4', '6.48', '6.56', '6.64', '6.72',
    '6.8', '6.88', '6.96', '7.04', '7.12', '7.2','7.28', '7.36', '7.44', '7.52', '7.6', '7.68', '7.76', '7.84', '7.92'] #Heading for MMD Data

# Adjust Data for VA features
VA_features = VA_Data[['Unnamed: 1','Unnamed: 2','Unnamed: 3']] #Vinyl Acetate MMD Features
VA_features.insert(2, 'Temperature', 60) #Manually add temperature data; all simulations take place at 60C per reference
VA_features['Label'] = 'VA' #Add label to all Vinyl Acetate Data
VA_features.columns= [ 'Monomer', 'Initiator','Temperature','Time','Label'] #Ensure Proper Column Titles, 
VA_features.drop(index=VA_features.index[0], inplace=True)

#Ensure standardization for Data
VA_Data= VA_Data.drop(columns=['Unnamed: 1','Unnamed: 2','Unnamed: 3', 'Unnamed: 4',	'Unnamed: 5',	'logM']) #We are left with just MMD Data
VA_Data.drop(index=VA_Data.index[0], inplace=True) #removes top line of unnecessary text from the dataframe
VA_Data['0'] = 0 # Manually add 0 row for the MMD data to ensure uniformity between the two, this set starts at 0.08
VA_Data.columns = ['0', '0.08', '0.16', '0.24', '0.32', '0.4', '0.48', '0.56', '0.64', '0.72', '0.8','0.88', '0.96', '1.04', '1.12', '1.2', '1.28', 
    '1.36', '1.44', '1.52', '1.6','1.68', '1.76', '1.84', '1.92', '2.0', '2.08', '2.16', '2.24', '2.32', '2.4','2.48', '2.56', '2.64',
    '2.72', '2.80', '2.88', '2.96', '3.04', '3.12', '3.2','3.28', '3.36', '3.44', '3.52', '3.6', '3.68', '3.76', '3.84', '3.92', '4.0',
    '4.08', '4.16', '4.24', '4.32', '4.4', '4.48', '4.56', '4.64', '4.72', '4.8','4.88', '4.96', '5.04', '5.12', '5.2', '5.28', '5.36', 
    '5.44', '5.52', '5.6', '5.68', '5.76', '5.84', '5.92', '6.0', '6.08', '6.16', '6.24', '6.32', '6.4', '6.48', '6.56', '6.64', '6.72',
    '6.8', '6.88', '6.96', '7.04', '7.12', '7.2','7.28', '7.36', '7.44', '7.52', '7.6', '7.68', '7.76', '7.84', '7.92']

#Combine Data into Singular feature set
Combined_Features = pd.concat([BA_Features, VA_features], axis=0, ignore_index=True) # X Values
Combined_MMD_Data = pd.concat([BA_MMD_Data, VA_Data], axis=0, ignore_index=True) # Y Values

#==============REGRESSION AND MACHINE LEARNING (SECTION 1) ===============
## Butyl Acrylate REACTION DATA

#Train/Test split used twice in order to introduce a validation set and a training set
#This is broken up so that 20% is the Test set, 70% is the Training Set and 10% is the Validation set
X_train_val, X_test, y_train_val, y_test = train_test_split(features,Reaction_data , test_size=0.2, random_state=42) 
X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.125, random_state=42)#10% of data is validation; 10/80=12.5

#========================================BA NEURAL NETWORK ================================#
#for the neural network all data had to be converted to a float
X_train_= X_train.astype(float)
y_train_= y_train.astype(float)
X_val_= X_val.astype(float)
y_val_= y_val.astype(float)
X_test_= X_test.astype(float)
y_test_= y_test.astype(float)
#Scaling the Values helps to improve performance of the NN
scaler= StandardScaler()
X_train_scaler= scaler.fit_transform(X_train)
X_val_scaler= scaler.transform(X_val)
X_test_scaler= scaler.transform(X_test)
#converting to pytorch tensors before input to NN
X_train_tensor= torch.tensor(X_train_scaler, dtype=torch.float32)
y_train_tensor= torch.tensor(y_train.to_numpy(), dtype=torch.float32)
X_val_tensor= torch.tensor(X_val_scaler, dtype=torch.float32)
y_val_tensor= torch.tensor(y_val.to_numpy(), dtype=torch.float32)
X_test_tensor= torch.tensor(X_test_scaler, dtype=torch.float32)
y_test_tensor= torch.tensor(y_test.to_numpy(), dtype=torch.float32)

def neuralnetwork(input_size, output_size):
    return nn.Sequential(nn.Linear(input_size, 300), nn.ReLU(), # FIRST LAYER
            nn.Linear(300, 150),nn.ReLU(), # SECOND LAYER
            nn.Linear(150, 75),nn.ReLU(),       # THIRD LAYER
            nn.Linear(75, output_size) )   #RESULTS
    
input_size= X_train_tensor.shape[1]
output_size= y_train_tensor.shape[1]
BA_model= neuralnetwork(input_size, output_size) #BA model untrained

adjustment= nn.MSELoss() #loss calculation method
optimize= optim.Adam(BA_model.parameters()) #optimizes parameters within the training loop


# Training loop
i= 150000
for iteration in range(i):
    BA_model.train() #model
    predictions= BA_model(X_train_tensor) #predicted value
    loss= adjustment(predictions, y_train_tensor) #difference between the prediction and the actual values
    optimize.zero_grad() #clears from the previous loop
    loss.backward() #computes parameter based on the loss
    optimize.step() #updates parameters going forward
    if (iteration+1) % 10 == 0: #print to understand training progress
        print(iteration+1, 'of', i)
        print (loss)

#Evaluate model
#Validation set
BA_model.eval()
with torch.no_grad():
    y_pred_val= BA_model(X_val_tensor)

#Test set
BA_model.eval()
with torch.no_grad():
    y_pred= BA_model(X_test_tensor)

#Plotting and analysis required numpy arrays
y_pred_numpy = y_pred.numpy()
y_pred_val_numpy= y_pred_val.numpy()
y_test_numpy = y_test_tensor.numpy()
#========================================BA NEURAL NETWORK END================================#

#========================================BA RANDOM FOREST================================#
# Create a MultiOutputRegressor with a Random Forest using the the base estimator.
multi_rf_model= MultiOutputRegressor(RandomForestRegressor())
multi_rf_model.fit(X_train, y_train) #train the data against the training set

y_pred_forest = multi_rf_model.predict(X_test) #prediction on test set
y_pred_val_forest=multi_rf_model.predict(X_val) #prediction on validation set

#transition to dataframe for analysis
y_actual= pd.DataFrame(y_test.values, columns=y_test.columns)
y_actual_validation= pd.DataFrame(y_val.values, columns=y_test.columns)
y_predicted= pd.DataFrame(y_pred_forest, columns=y_test.columns)
y_predicted_validation= pd.DataFrame(y_pred_val_forest, columns=y_test.columns)
#========================================BA RANDOM FOREST END================================#

#Given the predicted values, we will convert them all to MW, MN, and PDI to better compare accuracy between models
Molecular_weights = pd.DataFrame()
Molecular_weights['MN_test_avg'] = y_actual.iloc[:, 1:12].mean(axis=1) #Actual
Molecular_weights['MN_RF_avg'] = y_predicted.iloc[:, 1:12].mean(axis=1) #RF
Molecular_weights['MN_NN_avg'] = y_pred_numpy[:, 1:12].mean(axis=1) #NN

Molecular_weights['MW_test_avg'] = y_actual.iloc[:, 13:24].mean(axis=1) #Actual
Molecular_weights['MW_RF_avg'] = y_predicted.iloc[:, 13:24].mean(axis=1) #RF
Molecular_weights['MW_NN_avg'] = y_pred_numpy[:, 13:24].mean(axis=1) #NN

Molecular_weights['PDI_test_avg'] = Molecular_weights['MW_test_avg']/Molecular_weights['MN_test_avg'] #Actual
Molecular_weights['PDI_RF_avg'] = Molecular_weights['MW_RF_avg']/Molecular_weights['MN_RF_avg'] #RF
Molecular_weights['PDI_NN_avg'] = Molecular_weights['MW_NN_avg']/Molecular_weights['MN_NN_avg'] #NN

#plotting of different parameters below using both models
plt.figure()
plt.plot(y_actual.iloc[:,0], y_actual.iloc[:,0], color='red', alpha=0.8, linestyle='dashed', label="perfect prediction ")
plt.scatter(y_actual.iloc[:,0], y_predicted.iloc[:,0], color="green", alpha=0.5, label="Random Forest ")
plt.scatter(y_test_numpy[:, 0], y_pred_numpy[:,0], color='blue', label="Neural Network ")
plt.xlabel("Actual Reaction Time")
plt.ylabel("Predicted Reaction Time ")
plt.title("Prediciton of Reaction Time in Various Models")
plt.legend()
plt.show()

plt.figure()
plt.scatter(Molecular_weights.iloc[:,6],Molecular_weights.iloc[:,7], color='green', alpha=0.8, label="Random Forest")
plt.scatter(Molecular_weights.iloc[:,6], Molecular_weights.iloc[:,8], color="blue", alpha=0.5, label="Neural Network")
plt.plot(Molecular_weights.iloc[:,6], Molecular_weights.iloc[:,6], color='red', linestyle='dashed', label="Perfect Prediction")
plt.xlabel("Actual PDI")
plt.ylabel("Predicted PDI (Target")
plt.title("Actual vs Predicted for PDI")
plt.legend()
plt.show()

plt.figure()
plt.plot(y_actual.iloc[:,25:].mean(axis=1), y_actual.iloc[:,25:].mean(axis=1), color='red', alpha=0.8, linestyle='dashed', label="perfect prediction ")
plt.scatter(y_actual.iloc[:,25:].mean(axis=1), y_predicted.iloc[:,25:].mean(axis=1), color="green", alpha=0.5, label="Random Forest ")
plt.scatter(y_test_numpy[:,25], y_pred_numpy[:,25], color='blue', label="Neural Network ")
plt.xlabel("Actual BA Concentration")
plt.ylabel("Predicted Concentration  ")
plt.title("Prediciton of Butyl Acrylate Concentration in Various Models")
plt.legend()
plt.show()

# We are using r^2 and Mean squared error (MSE) to evaluate model success
#Statistical parameters for reaction time
# Random forest
mse = mean_squared_error(y_actual.iloc[:,0], y_predicted.iloc[:,0])
r2 = r2_score(y_actual.iloc[:,0], y_predicted.iloc[:,0])
print("RF statistics for Reaction time")
print ()
print("Reaction Time MSE", mse)
print("Reaction Time R^2", r2)

# Neural Network 
mse = mean_squared_error(y_test_numpy[:, 0], y_pred_numpy[:, 0])
r2 = r2_score(y_test_numpy[:, 0], y_pred_numpy[:, 0])
print("NN statistics for Reaction time")
print ()
print("Reaction Time MSE", mse)
print("Reaction Time R^2", r2)

#Statistical parameters for PDI
# Random forest
mse = mean_squared_error(Molecular_weights.iloc[:,6],Molecular_weights.iloc[:,7])
r2 = r2_score(Molecular_weights.iloc[:,6],Molecular_weights.iloc[:,7])
print("RF statistics for PDI ")
print ()
print("PDI  MSE", mse)
print("PDI  R^2", r2)

# Neural Network 
mse = mean_squared_error(Molecular_weights.iloc[:,6], Molecular_weights.iloc[:,8])
r2 = r2_score(Molecular_weights.iloc[:,6], Molecular_weights.iloc[:,8])
print("NN statistics for PDI")
print ()
print("PDI MSE", mse)
print("PDI R^2", r2)

#Statistical parameters for BA
# Random forest
mse = mean_squared_error(y_actual.iloc[:,25], y_predicted.iloc[:,25])
r2 = r2_score(y_actual.iloc[:,25], y_predicted.iloc[:,25])
print("RF statistics for BA ")
print ()
print("BA MSE", mse)
print("BA R^2", r2)

# Neural Network 
mse = mean_squared_error(y_test_numpy[:,25], y_pred_numpy[:,25])
r2 = r2_score(y_test_numpy[:,25], y_pred_numpy[:,25])
print("NN statistics for BA")
print ()
print("BA MSE", mse)
print("BA R^2", r2)

#######
#######
#######================================SECTION 2=================
#######
#######
#______MMD ANALYSIS______

MMD_features= Combined_Features.drop(columns=['Label'])
#Train/Test split used twice in the exact manner used in part 1 to generate 70:10:20 Train:Validate:Test ratio
X_train_val_MMD, X_test_MMD, y_train_val_MMD, y_test_MMD = train_test_split(MMD_features, Combined_MMD_Data, test_size=0.2, random_state=42) 
X_train_MMD, X_val_MMD, y_train_MMD, y_val_MMD = train_test_split(X_train_val_MMD, y_train_val_MMD, test_size=0.125, random_state=42)#10% of data is validation; 10/80=12.5

# FIRST WE WILL TRAIN USING A RANDOM FOREST MODEL
#========================================MMD RANDOM FOREST ================================#
forest_model= MultiOutputRegressor(RandomForestRegressor()) #Model uses the standard 100 estimators
forest_model.fit(X_train_MMD, y_train_MMD) #model is trained using the X and Y training data

# Evaluate on validation set 
y_val_pred_RF = forest_model.predict(X_val_MMD)

# Evaluate on test set
y_test_pred_RF = forest_model.predict(X_test_MMD)
#========================================MMD RANDOM FOREST END================================#

#========================================MMD NEURAL NETWORK================================#
#NEURAL NETWORK EVALUATION OF MMD
# Neural Network data is processed in same manner as section one, starting with float, then transforming
# X values  to scalars and finally converting them all to torch tensors
X_train_MMD= X_train_MMD.astype(float)
y_train_MMD= y_train_MMD.astype(float)
X_val_MMD= X_val_MMD.astype(float)
y_val_MMD= y_val_MMD.astype(float)
X_test_MMD= X_test_MMD.astype(float)
y_test_MMD= y_test_MMD.astype(float)

scaler = StandardScaler()
X_train_scaler= scaler.fit_transform(X_train_MMD)
X_val_scaler= scaler.transform(X_val_MMD)
X_test_scaler= scaler.transform(X_test_MMD)

X_train_tensor= torch.tensor(X_train_scaler, dtype=torch.float32)
y_train_tensor= torch.tensor(y_train_MMD.to_numpy(), dtype=torch.float32)
X_val_tensor= torch.tensor(X_val_scaler, dtype=torch.float32)
y_val_tensor= torch.tensor(y_val_MMD.to_numpy(), dtype=torch.float32)
X_test_tensor= torch.tensor(X_test_scaler, dtype=torch.float32)
y_test_tensor= torch.tensor(y_test_MMD.to_numpy(), dtype=torch.float32)

# We will use the same neural network function used in Section 1

input_size = X_train_tensor.shape[1]
output_size = y_train_tensor.shape[1]
MMD_model = neuralnetwork(input_size, output_size)

optimize = optim.Adam(MMD_model.parameters()) #similar optimization as above
adjustment=nn.MSELoss() #similar loss function as above
# Training loop for MMD
i= 5000

#training loop is set up in the same way as above
for iteration in range(i):
    MMD_model.train()
    predictions= MMD_model(X_train_tensor)
    loss= adjustment(predictions, y_train_tensor)
    optimize.zero_grad()
    loss.backward()
    optimize.step()
    if (iteration+1) % 10 == 0:
        print(iteration+1, 'of', i)
        print (loss)


# Evaluate NN on validation set
MMD_model.eval()
with torch.no_grad():
    y_pred_val = MMD_model(X_test_tensor)
# Evaluate NN on test set
MMD_model.eval()
with torch.no_grad():
    y_pred = MMD_model(X_test_tensor)

#numpy arrays needed for plotting and analysis
y_pred_numpy = y_pred.numpy()
y_pred_val_numpy= y_pred_val.numpy()
y_test_numpy = y_test_tensor.numpy()
#========================================MMD NEURAL NETWORK END================================#

##_______________________Understandin Polymerization Kinetics based on MMD_________________________

#explanation of the calculation below can be found in appendix A of the accompyaning report
def MW_MN_calculation(mmd_vector,columns):
    count= np.array(mmd_vector, dtype=float)
    total= count.sum()
    if total== 0:
        return (0.0, 0.0, 0.0)
    Mn= np.sum(count*columns)/total
    Mw= np.sum(count*columns**2/np.sum(count*columns))
    PDI= Mw/Mn if Mn != 0 else 0.0
    return Mn, Mw, PDI

y_test_array= y_test_MMD.values 
mw_bins = np.array([float(col) for col in Combined_MMD_Data.columns]) 
actual_mw= [MW_MN_calculation(row, mw_bins) for row in y_test_array]
actual_mw_ = pd.DataFrame(actual_mw, columns=["Mn_actual", "Mw_actual", "PDI_actual"])
# Random forest predictions
predicted_mw= [MW_MN_calculation(row, mw_bins) for row in y_test_pred_RF]
predicted_mw_= pd.DataFrame(predicted_mw, columns=["Mn_pred", "Mw_pred", "PDI_pred"])
# NN predictions
predicted_mw_NN = [MW_MN_calculation(row, mw_bins) for row in y_pred_numpy]
predicted_mw_NN_ = pd.DataFrame(predicted_mw_NN, columns=["Mn_pred", "Mw_pred", "PDI_pred"])

#plot the Predictions of MMD
plt.figure(figsize=(8, 6))
plt.plot(actual_mw_.iloc[:, 2], actual_mw_.iloc[:, 2], color='red', alpha=0.8, label="Test Data")
plt.scatter(actual_mw_.iloc[:, 2], predicted_mw_NN_.iloc[:, 2], color='blue', alpha=0.8, label="Neural Network ")
plt.scatter(actual_mw_.iloc[:, 2], predicted_mw_.iloc[:, 2], color='green', alpha=0.5, label="Random Forest")
plt.xlabel("Actual PDI")
plt.ylabel("Predicted PDI")
plt.title("Effect of Different Models on MMD (PDI)")
plt.legend()
plt.show()

#statistical explanation of MMD Data
print(" MMD Prediction (Neural network) on Test Set")
r2= r2_score(y_test_numpy, y_pred_numpy)
mse= mean_squared_error(y_test_numpy, y_pred_numpy)
print ()
print("BA MSE", mse)
print("BA R^2", r2)

print("MMD Prediction (Random Forest) on Test Set")
r2 = r2_score(y_test_MMD, y_test_pred_RF)
mse = mean_squared_error(y_test_MMD, y_test_pred_RF)
print ()
print("BA MSE", mse)
print("BA R^2", r2)

#####================\

#Use the predicted MMD Data to reverse engineer original labels

x_classifier= Combined_MMD_Data
y_classifier= Combined_Features['Label']

x_train_classifier, x_test_classifier, y_train_classifier, y_test_classifier = train_test_split(x_classifier, y_classifier, test_size=0.2, random_state=42)

# Initialize and train a Random Forest Classifier.
rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)
rf_classifier.fit(x_train_classifier, y_train_classifier)

# Make predictions on the test set.
y_pred_cls = rf_classifier.predict(x_test_classifier)

# Compute the confusion matrix.
rf_matrix= confusion_matrix(y_test_classifier, y_pred_cls)

plt.figure(figsize=(8, 6))
sns.heatmap(rf_matrix, annot=True, fmt='d', cmap='Blues',
            xticklabels=rf_classifier.classes_, yticklabels=rf_classifier.classes_)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Rf_matrix (Random Forest)')
plt.show()

#accuracy 
numerator=np.trace(rf_matrix)
denominator=np.sum(rf_matrix)
accuracy=numerator/denominator
print("accuracy")
print(accuracy)

#false positive
FP=rf_matrix[0,1]
TP=rf_matrix[0,0]
FN=rf_matrix[1,0]
FPR=FP/(TP+FN)
print("FPR")
print(FPR)

#false negative = 1-TPR
TPR=TP/(TP+FN)
FNR=1-TPR
print("FNR")
print(FNR)

#Ensure all information can be used in neural network

X_train_classify_numeric = x_train_classifier.apply(pd.to_numeric, errors='coerce')
X_test_classify_numeric = x_test_classifier.apply(pd.to_numeric, errors='coerce')
X_train_classify_array = X_train_classify_numeric.to_numpy().astype(np.float32)
X_test_classify_array = X_test_classify_numeric.to_numpy().astype(np.float32)
X_train_classify_tensor = torch.tensor(X_train_classify_array, dtype=torch.float32)
X_test_classify_tensor = torch.tensor(X_test_classify_array, dtype=torch.float32)

y_train_classify_numeric = y_train_classifier.astype("category").cat.codes
y_test_classify_numeric = y_test_classifier.astype("category").cat.codes

#y_train_classify_numeric = y_train_classifier.apply(pd.to_numeric, errors='coerce')
#y_test_classify_numeric = y_test_classifier.apply(pd.to_numeric, errors='coerce')
y_train_classify_tensor = torch.tensor(y_train_classify_numeric.to_numpy().astype(np.int64), dtype=torch.long)
y_test_classify_tensor = torch.tensor(y_test_classify_numeric.to_numpy().astype(np.int64), dtype=torch.long)

input_size = X_train_classify_tensor.shape[1]
num_classes = len(torch.unique(y_train_classify_tensor))

classifier_model= neuralnetwork(input_size, num_classes)

optimize= optim.Adam(classifier_model.parameters())
adjustment= nn.CrossEntropyLoss()

i= 3000
for iteration in range(i):
    classifier_model.train()
    predictions= classifier_model(X_train_classify_tensor)
    loss= adjustment(predictions, y_train_classify_tensor)
    optimize.zero_grad()
    loss.backward()
    optimize.step()
    if (iteration+1) % 10 == 0:
        print(iteration+1, 'of', i)

classifier_model.eval()
with torch.no_grad():
    test_outputs = classifier_model(X_test_classify_tensor)
    _, predicted_classify = torch.max(test_outputs, 1)

# Convert tensors to numpy arrays for the confusion matrix.
y_test_np = y_test_classify_tensor.cpu().numpy()
predicted_np = predicted_classify.cpu().numpy()

# Generate confusion matrix and classification report.
NN_matrix= confusion_matrix(y_test_np, predicted_np)
plt.figure(figsize=(8, 6))
sns.heatmap(NN_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=rf_classifier.classes_, yticklabels=rf_classifier.classes_)
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix (Neural Network)')
plt.show()
